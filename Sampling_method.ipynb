{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cm1jrHgFZ0kB",
        "outputId": "873875af-3302-4aed-e26d-33d50a9e165e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3071746889.py:45: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  return df.groupby(\"Class\", group_keys=False).apply(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy Table:\n",
            "   Sampling1_SimpleRandom Sampling2_Stratified Sampling3_Systematic  \\\n",
            "M1                  89.86                92.03                88.89   \n",
            "M2                  96.38                97.83                98.69   \n",
            "M3                  97.83                100.0                100.0   \n",
            "M4                  67.39                70.29                 71.9   \n",
            "M5                  94.93                99.28                95.42   \n",
            "\n",
            "   Sampling4_Cluster Sampling5_Bootstrap  \n",
            "M1             96.83               92.03  \n",
            "M2             99.21               97.83  \n",
            "M3             100.0               100.0  \n",
            "M4             100.0               77.54  \n",
            "M5             99.21                97.1  \n",
            "\n",
            "Best Sampling Technique for Each Model:\n",
            "M1       Sampling4_Cluster\n",
            "M2       Sampling4_Cluster\n",
            "M3    Sampling2_Stratified\n",
            "M4       Sampling4_Cluster\n",
            "M5    Sampling2_Stratified\n",
            "dtype: object\n"
          ]
        }
      ],
      "source": [
        "# ============================================\n",
        "# Sampling Assignment â€“ FINAL FULL CODE\n",
        "# ============================================\n",
        "\n",
        "# 1. IMPORT LIBRARIES\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from sklearn.utils import resample\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "\n",
        "# 2. LOAD DATASET\n",
        "data = pd.read_csv(\n",
        "    \"https://raw.githubusercontent.com/AnjulaMehto/Sampling_Assignment/main/Creditcard_data.csv\"\n",
        ")\n",
        "\n",
        "X = data.drop(\"Class\", axis=1)\n",
        "y = data[\"Class\"]\n",
        "\n",
        "\n",
        "# 3. BALANCE DATASET\n",
        "ros = RandomOverSampler(random_state=42)\n",
        "X_bal, y_bal = ros.fit_resample(X, y)\n",
        "\n",
        "balanced_df = pd.concat([X_bal, y_bal], axis=1)\n",
        "\n",
        "\n",
        "# 4. SAMPLING FUNCTIONS\n",
        "def simple_random_sampling(df, frac=0.3, seed=42):\n",
        "    return df.sample(frac=frac, random_state=seed)\n",
        "\n",
        "def stratified_sampling(df, frac=0.3, seed=42):\n",
        "    return df.groupby(\"Class\", group_keys=False).apply(\n",
        "        lambda x: x.sample(frac=frac, random_state=seed)\n",
        "    )\n",
        "\n",
        "def systematic_sampling(df, step=3):\n",
        "    return df.iloc[::step]\n",
        "\n",
        "def cluster_sampling(df, n_clusters=10, clusters_to_pick=3, seed=42):\n",
        "    features = df.drop(\"Class\", axis=1)\n",
        "    kmeans = KMeans(n_clusters=n_clusters, random_state=seed)\n",
        "    df = df.copy()\n",
        "    df[\"cluster\"] = kmeans.fit_predict(features)\n",
        "\n",
        "    chosen_clusters = np.random.RandomState(seed).choice(\n",
        "        n_clusters, clusters_to_pick, replace=False\n",
        "    )\n",
        "    sampled_df = df[df[\"cluster\"].isin(chosen_clusters)]\n",
        "    return sampled_df.drop(\"cluster\", axis=1)\n",
        "\n",
        "def bootstrap_sampling(df, n_samples, seed=42):\n",
        "    return df.sample(n=n_samples, replace=True, random_state=seed)\n",
        "\n",
        "\n",
        "# 5. CREATE SAMPLES USING REQUIRED TECHNIQUES\n",
        "samples = {\n",
        "    \"Sampling1_SimpleRandom\": simple_random_sampling(balanced_df, frac=0.3, seed=42),\n",
        "    \"Sampling2_Stratified\": stratified_sampling(balanced_df, frac=0.3, seed=42),\n",
        "    \"Sampling3_Systematic\": systematic_sampling(balanced_df, step=3),\n",
        "    \"Sampling4_Cluster\": cluster_sampling(balanced_df, n_clusters=10, clusters_to_pick=3, seed=42),\n",
        "    \"Sampling5_Bootstrap\": bootstrap_sampling(\n",
        "        balanced_df, n_samples=int(0.3 * len(balanced_df)), seed=42\n",
        "    )\n",
        "}\n",
        "\n",
        "\n",
        "# 6. MACHINE LEARNING MODELS\n",
        "models = {\n",
        "    \"M1\": LogisticRegression(max_iter=5000),\n",
        "    \"M2\": DecisionTreeClassifier(),\n",
        "    \"M3\": RandomForestClassifier(),\n",
        "    \"M4\": GaussianNB(),\n",
        "    \"M5\": SVC()\n",
        "}\n",
        "\n",
        "\n",
        "# 7. APPLY ALL SAMPLINGS ON ALL MODELS\n",
        "results = pd.DataFrame(\n",
        "    index=models.keys(),\n",
        "    columns=samples.keys()\n",
        ")\n",
        "\n",
        "for samp_key, samp_df in samples.items():\n",
        "\n",
        "    X = samp_df.drop(\"Class\", axis=1)\n",
        "    y = samp_df[\"Class\"]\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.3, random_state=42\n",
        "    )\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "    for model_key, model in models.items():\n",
        "\n",
        "        if model_key in [\"M1\", \"M5\"]:\n",
        "            model.fit(X_train_scaled, y_train)\n",
        "            y_pred = model.predict(X_test_scaled)\n",
        "        else:\n",
        "            model.fit(X_train, y_train)\n",
        "            y_pred = model.predict(X_test)\n",
        "\n",
        "        acc = accuracy_score(y_test, y_pred) * 100\n",
        "        results.loc[model_key, samp_key] = round(acc, 2)\n",
        "\n",
        "\n",
        "# 8. DISPLAY RESULTS\n",
        "print(\"Accuracy Table:\")\n",
        "print(results)\n",
        "\n",
        "\n",
        "# 9. BEST SAMPLING PER MODEL\n",
        "best_sampling = results.astype(float).idxmax(axis=1)\n",
        "print(\"\\nBest Sampling Technique for Each Model:\")\n",
        "print(best_sampling)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0ycCe7JVZ8yG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}